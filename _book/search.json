[
  {
    "objectID": "parallelization on workbench.html",
    "href": "parallelization on workbench.html",
    "title": "3  Parallelization on Workbench",
    "section": "",
    "text": "3.1 What is Parallelization?\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can have multiple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here is an example of four quad-core processors for a total of 16 cores in this machine.\nYou can think of this as allowing 16 computations to happen at the same time. Theoretically, your computation would take 1/16 of the time. Historically, R has only utilized one processor, which makes it single-threaded.",
    "crumbs": [
      "R tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization on workbench.html#the-lapply-function",
    "href": "parallelization on workbench.html#the-lapply-function",
    "title": "3  Parallelization on Workbench",
    "section": "3.2 The lapply() function",
    "text": "3.2 The lapply() function\nThe lapply() function has two arguments:\n\nA list, or an object that can be coerced to a list.\nA function to be applied to each element of the list\n\nThe lapply() function works much like a loop. It cycles through each element of the list and applies the supplied function to that element. While lapply() is applying your function to a list element, the other elements of the list are just…sitting around in memory. In the description of lapply(), there’s no mention of the different elements of the list communicating with each other, and the function being applied to a given list element does not need to know about other list elements.\nJust about any operation that is handled by the lapply() function can be parallelized. The idea is that a list object can be split across multiple cores of a processor and then the function can be applied to each subset of the list object on each of the cores. Conceptually, the steps in the parallel procedure are\n\nSplit list X across multiple cores\nCopy the supplied function (and associated environment) to each of the cores\nApply the supplied function to each subset of the list X on each of the cores in parallel\nAssemble the results of all the function evaluations into a single list and return\n\nThe differences between the many packages/functions in R essentially come down to how each of these steps are implemented.",
    "crumbs": [
      "R tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization on workbench.html#the-parallel-package",
    "href": "parallelization on workbench.html#the-parallel-package",
    "title": "3  Parallelization on Workbench",
    "section": "3.3 The Parallel Package",
    "text": "3.3 The Parallel Package\nThe parallel package which comes with your R installation.\nThe mclapply() function essentially parallelizes calls to lapply(). The first two arguments to mclapply() are exactly the same as they are for lapply(). However, mclapply() has further arguments (that must be named), the most important of which is the mc.cores argument which you can use to specify the number of processors/cores you want to split the computation across. For example, if your machine has 4 cores on it, you might specify mc.cores = 4 to break your parallelize your operation across 4 cores (although this may not be the best idea if you are running other operations in the background besides R).\nBriefly, your R session is the main process and when you call a function like mclapply(), you fork a series of sub-processes that operate independently from the main process (although they share a few low-level features). These sub-processes then execute your function on their subsets of the data, presumably on separate cores of your CPU. Once the computation is complete, each sub-process returns its results and then the sub-process is killed. The parallel package manages the logistics of forking the sub-processes and handling them once they’ve finished.\n\n\n\n\n\n\nCaution\n\n\n\nBecause of the use of the fork mechanism, the mc* functions are generally not available to users of the Windows operating system.\n\n\nThe first thing you might want to check with the parallel package is if your computer in fact has multiple cores that you can take advantage of.\n\nlibrary(parallel)\nparallel::detectCores()\n\n[1] 64\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn general, the information from detectCores() should be used cautiously as obtaining this kind of information from Unix-like operating systems is not always reliable. If you are going down this road, it’s best if you get to know your hardware better in order to have an understanding of how many CPUs/cores are available to you.\n\n\n\n3.3.1 mclapply()\n\nset.seed(1)\n# Create a dataframe\ndf &lt;- data.frame(replicate(1000, rnorm(10000)))\n\n# Using lapply() to find mean of each row\ns &lt;- system.time({\n  list_means_1 &lt;- lapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])))\n})\nprint(s)\n\n   user  system elapsed \n 44.442   0.001  44.466 \n\n\nNote that in the system.time() output in first case, the user time and the elapsed time are roughly the same, which is what we would expect because there was no parallelization.\n\nlibrary(parallel)\n\n# Using mclapply() to find mean of each row\nnumberOfCores &lt;- 4\ns &lt;- system.time({\n  list_means_2 &lt;- parallel::mclapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])), mc.cores = numberOfCores)\n})\nprint(s)\n\n   user  system elapsed \n 34.987   0.314  11.968 \n\n\nYou’ll notice that the the elapsed time is now less than the user time. However, in general, the elapsed time will not be 1/4th of the user time, which is what we might expect with 4 cores if there were a perfect performance gain from parallelization.\nR keeps track of how much time is spent in the main process and how much is spent in any child processes.\n\ns[\"user.self\"]  # Main process\n\nuser.self \n    0.003 \n\ns[\"user.child\"] # Child processes\n\nuser.child \n    34.984 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nOne advantage of serial computations is that it allows you to better keep a handle on how much memory your R job is using. When executing parallel jobs via mclapply() it’s important to pre-calculate how much memory all of the processes will require and make sure this is less than the total amount of memory on your computer.\n\n\nThe mclapply() function is useful for iterating over a single list or list-like object. If you have to iterate over multiple objects together, you can use mcmapply(), which is the the multi-core equivalent of the mapply() function.\n\n\n3.3.2 Error Handling\nThis error handling behavior is a significant difference from the usual call to lapply(). With lapply(), if the supplied function fails on one component of the list, the entire function call to lapply() fails and you only get an error as a result.\nWith mclapply(), when a sub-process fails, the return value for that sub-process will be an R object that inherits from the class \"try-error\", which is something you can test with the inherits() function. Conceptually, each child process is executed with the try() function wrapped around it. The code below deliberately causes an error in the 3 element of the list.\n\nr &lt;- parallel::mclapply(1:5, function(i) {\n        if(i == 3L)\n                stop(\"error in this process!\")\n        else\n                return(\"success!\")\n}, mc.cores = 5)\n\nWarning in parallel::mclapply(1:5, function(i) {: scheduled core 3 encountered\nerror in user code, all values of the job will be affected\n\n\nHere we see there was a warning but no error in the running of the above code. We can check the return value.\n\nstr(r)\n\nList of 5\n $ : chr \"success!\"\n $ : chr \"success!\"\n $ : 'try-error' chr \"Error in FUN(X[[i]], ...) : error in this process!\\n\"\n  ..- attr(*, \"condition\")=List of 2\n  .. ..$ message: chr \"error in this process!\"\n  .. ..$ call   : language FUN(X[[i]], ...)\n  .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n $ : chr \"success!\"\n $ : chr \"success!\"\n\n\nNote that the 3rd list element in r is different.\n\nclass(r[[3]])\n\n[1] \"try-error\"\n\ninherits(r[[3]], \"try-error\")\n\n[1] TRUE\n\n\nWhen running code where there may be errors in some of the sub-processes, it’s useful to check afterwards to see if there are any errors in the output received.\n\nbad &lt;- sapply(r, inherits, what = \"try-error\")\nbad\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n\n3.3.3 Generating Random Numbers\n\nset.seed(1)\nr &lt;- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n\nList of 5\n $ : num [1:3] 0.726 -0.76 -1.721\n $ : num [1:3] -0.536 1.167 1.224\n $ : num [1:3] 1.32 0.33 -1.31\n $ : num [1:3] -1.17 -1.59 1.35\n $ : num [1:3] 0.968 -1.08 1.575\n\n\nHowever, the above expression is not reproducible because the next time you run it, you will get a different set of random numbers. You cannot simply call set.seed() before running the expression as you might in a non-parallel version of the code.\nThe parallel package provides a way to reproducibly generate random numbers in a parallel environment via the “L’Ecuyer-CMRG” random number generator. Note that this is not the default random number generator so you will have to set it explicitly.\n\nRNGkind(\"L'Ecuyer-CMRG\")\nset.seed(1)\nr &lt;- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n\nList of 5\n $ : num [1:3] -0.485 -0.626 -0.873\n $ : num [1:3] -1.86 -1.825 -0.995\n $ : num [1:3] 1.177 1.472 -0.988\n $ : num [1:3] 0.984 1.291 0.459\n $ : num [1:3] 1.43 -1.137 0.844\n\n\nmclapply() documentation can be found here: mcapply()\n\n\n3.3.4 The ParLapply() function\nUsing the forking mechanism on your computer is one way to execute parallel computation but it’s not the only way that the parallel package offers. Another way to build a “cluster” using the multiple cores on your computer is via sockets. A is simply a mechanism with which multiple processes or applications running on your computer (or different computers, for that matter) can communicate with each other. With parallel computation, data and results need to be passed back and forth between the parent and child processes and sockets can be used for that purpose.\n\nclu &lt;- parallel::makeCluster(4)\n\nThe clu object is an abstraction of the entire cluster and is what we’ll use to indicate to the various cluster functions that we want to do parallel computation.\nTo do an lapply() operation over a socket cluster we can use the parLapply() function.\n\nlist_means_3 &lt;- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n\nError in checkForRemoteErrors(val): 4 nodes produced errors; first error: object of type 'closure' is not subsettable\n\n\nUnfortunately, that there’s an error in running this code. The reason is that while we have loaded the df data into our R session, the data is not available to the independent child processes that have been spawned by the makeCluster() function. The socket approach launches a new version of R on each core whereas the forking approach copies the entire current version of R and moves it to a new core.\nThe data, and any other information that the child process will need to execute your code, needs to be exported to the child process from the parent process via the clusterExport() function. The need to export data is a key difference in behavior between the “multicore” approach and the “socket” approach.\n\nparallel::clusterExport(clu, \"df\")\n\nThe second argument to clusterExport() is a character vector, and so you can export an arbitrary number of R objects to the child processes. You should be judicious in choosing what you export simply because each R object will be replicated in each of the child processes, and hence take up memory on your computer.\n\nlist_means_3 &lt;- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n\nOnce you’ve finished working with your cluster, it’s good to clean up and stop the cluster child processes (quitting R will also stop all of the child processes).\n\nparallel::stopCluster(clu)\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes we will also need to load the packages in individual child processes. This can be done by using clusterEvalQ . For example:\nparallel::clusterEvalQ(clu, {\n  library(ggplot2)\n  library(stringr)\n})\n\n\nParLapply() is a part of clusterApply() family of functions. The documentation can be found here: clusterApply()",
    "crumbs": [
      "R tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization on workbench.html#foreach-and-doparallel-package",
    "href": "parallelization on workbench.html#foreach-and-doparallel-package",
    "title": "3  Parallelization on Workbench",
    "section": "3.4 foreach and doParallel Package",
    "text": "3.4 foreach and doParallel Package\nThe normal for loop in R looks like:\n\nfor (i in 1:3) {\n  print(sqrt(i))\n}\n\n[1] 1\n[1] 1.414214\n[1] 1.732051\n\n\nThe foreach method is similar, but uses the sequential %do% operator to indicate an expression to run. Note the difference in the returned data structure.\n\nlibrary(foreach)\nforeach (i=1:3) %do% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n\n\n3.4.1 %dopar% operator\nIn addition, foreach supports a parallelizable operator %dopar% from the doParallel package. This allows each iteration through the loop to use different cores or different machines in a cluster.\n\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\ndoParallel::registerDoParallel(4) \nforeach (i=1:5) %dopar% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 2.236068\n\n\nTo simplify output, foreach has the .combine parameter that can simplify return values\n\nforeach (i=1:3, .combine=c) %dopar% {\n  sqrt(i)\n}\n\n[1] 1.000000 1.414214 1.732051\n\n\nforeach also has the .rbind parameter that can return a dataframe\n\nforeach (i=1:3, .combine=rbind) %dopar% {\n  sqrt(i)\n}\n\n             [,1]\nresult.1 1.000000\nresult.2 1.414214\nresult.3 1.732051\n\n\nThe doParallel vignette on CRAN shows a much more realistic example, where one can use %dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined. Here use the iris data set to do a parallel bootstrap:\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %dopar% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 11.719   0.308   3.509 \n\n\nAnd compare that to what it takes to do the same analysis in serial:\n\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %do% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 12.525   0.001  12.526 \n\n\nWhen we’re done, we will clean up the cluster:\n\ndoParallel::stopImplicitCluster()\n\n\n\n3.4.2 %dorng% operator\nstandard %dopar% loops are not reproducible:\nFirst, let’s set the RNGkind back to default\n\nRNGkind(\"default\")\n\nNow register a new cluster\n\ndoParallel::registerDoParallel(4)\n\n\nset.seed(123)\nres &lt;- foreach(i=1:5) %dopar% { runif(3) }\nset.seed(123)\nres2 &lt;- foreach(i=1:5) %dopar% { runif(3) }\nidentical(res, res2)\n\n[1] FALSE\n\n\nThe doRNG package provides convenient ways to implement reproducible parallel foreach loops, independently of the parallel backend used to perform the computation.\n\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nset.seed(123)\nres &lt;- foreach(i=1:5) %dorng% { runif(3) }\nset.seed(123)\nres2 &lt;- foreach(i=1:5) %dorng% { runif(3) }\nidentical(res, res2)\n\n[1] TRUE\n\n\nWhen we’re done, we will clean up the cluster:\n\ndoParallel::stopImplicitCluster()\n\nThe doParallel documentation can be found here: doParallel",
    "crumbs": [
      "R tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization on workbench.html#the-future-package",
    "href": "parallelization on workbench.html#the-future-package",
    "title": "3  Parallelization on Workbench",
    "section": "3.5 The future Package",
    "text": "3.5 The future Package\nThe future package defines plans to specify how computations are executed. A plan can use multiple cores, separate R sessions, or even remote systems.\n\nlibrary(future)\nplan(multisession, workers = 4)\n\n# Define a future\nf &lt;- future::future({ sum(1:1e6) })\n# Retrieve the result\nresult &lt;- future::value(f)\nprint(result)\n\n[1] 500000500000\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe could have used plan(multicore) instead of plan(multisession) if we were working directly in R and not RStudio. However, plan(multicore) will only work in a Linux/macOS environment\n\n\nThe future package is a lot more comprehensive but beyond the scope of discussion for this basic tutorial. If you are interested, the documentation can be found here: future package\nThis is how we can use it instead of lapply\n\nlibrary(future.apply)\nplan(multisession, workers = 4)\nresult &lt;- future.apply::future_lapply(1:5, function(x) x^2)\nresult\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nMultisession runs background R sessions on the current machine. For large parallelizations, we should run future_lapply by defining a cluster manually. That way we can run it on external R sessions on current, local, and/or remote machines.\n\nlibrary(parallel)\nclu &lt;- parallel::makeCluster(4)\nplan(cluster, workers = clu)\nresult &lt;- future_lapply(1:10, function(x) x^2)\nstopCluster(clu)\n\nDocumentation for future.apply family of functions can be found here: future.apply documentation\n\n3.5.1 Integration with SLURM\nFuture works very well with SLURM integration. The future.batchtools package extends the future ecosystem for SLURM. Here is an example code:\nlibrary(future.batchtools)\nplan(batchtools_slurm, template = \"sumbit_job.slurm\")\n\nf &lt;- future::future({ Sys.sleep(10); sum(1:1e6) })\nresult &lt;- future::value(f)\nYou need a SLURM template file (submit_job.slurm) that specifies job parameters (e.g., cores, memory).",
    "crumbs": [
      "R tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization on workbench.html#other-ways-to-parallelize-r-code",
    "href": "parallelization on workbench.html#other-ways-to-parallelize-r-code",
    "title": "3  Parallelization on Workbench",
    "section": "3.6 Other ways to parallelize R code",
    "text": "3.6 Other ways to parallelize R code\nThere are several other ways to parallelize your code. If you are looking for more packages, some of those are mentioned here along with their documentation. Some of these are extensions of packages already mentioned while others introduce different ways to parallelize.\n\nfurrr package\ndoFuture package\nRcppParallel package\nparallelMap package\nbatchtools package",
    "crumbs": [
      "R tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization on workbench.html#saving-results-while-parallelizing",
    "href": "parallelization on workbench.html#saving-results-while-parallelizing",
    "title": "3  Parallelization on Workbench",
    "section": "3.7 Saving results while parallelizing",
    "text": "3.7 Saving results while parallelizing\nWhen running large computations, it may be helpful to save results iteratively or as checkpoints to avoid data loss in case of interruptions. Here is an example of saving results iteratively using the foreach and doParallel libraries\n\nlibrary(foreach)\nlibrary(doParallel)\n\n# Register parallel backend\ndoParallel::registerDoParallel(4)\n\n# Parallel computation and saving results\nresults &lt;- foreach(i = 1:100, .combine = c) %dopar% {\n  # Your computation\n  Sys.sleep(0.1) # Simulates time-consuming computation\n  result &lt;- i^2\n  \n  # Save intermediate results\n  saveRDS(result, file = paste0(\"for_each/result_\", i, \".rds\"))\n  result\n}\n\ndoParallel::stopImplicitCluster()\n\nHere is another example using future.apply library\n\nlibrary(future.apply)\n\nplan(multisession, workers = 4)\n\n# Function with intermediate saving\nsafe_compute &lt;- function(i) {\n  result &lt;- i^2\n  saveRDS(result, file = paste0(\"future_apply/partial_result_\", i, \".rds\"))\n  return(result)\n}\n\n# Run computation\nresults &lt;- future.apply::future_lapply(1:100, safe_compute)\n\n# Aggregate saved results\nfinal_results &lt;- unlist(results)\nsaveRDS(final_results, file = \"future_apply/final_results.rds\")\n\n\n3.7.1 Designing a function to restore progress:\nHere is the “structure” of a function that can be used to restore your progress.\n\nlibrary(future.apply)\n\n# Define checkpoint directory\ncheckpoint_dir &lt;- \"checkpoints_parallel\"\ndir.create(checkpoint_dir, showWarnings = FALSE)\n\n# Set up parallel plan\nplan(multisession, workers = 4)\n\n# Function to perform computations with checkpoints\ncompute_task &lt;- function(i) {\n  checkpoint_file &lt;- file.path(checkpoint_dir, paste0(\"result_\", i, \".rds\"))\n  \n  if (file.exists(checkpoint_file)) {\n    # Restore from checkpoint\n    result &lt;- readRDS(checkpoint_file)\n  } else {\n    # Perform the computation\n    Sys.sleep(1)  # Simulates a time-consuming task\n    result &lt;- i*2\n    \n    # Save checkpoint\n    saveRDS(result, checkpoint_file)\n  }\n  return(result)\n}\n\n# Parallel computation with checkpoints\nresults &lt;- future.apply::future_lapply(1:10, compute_task)\n\n# Aggregate results\nfinal_results &lt;- unlist(results)\nprint(final_results)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\n\n3.7.2 Targets package for efficient checkpoint management\nThis package is a pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.\n\nlibrary(targets)\ntar_script({\n  library(future.apply)\n\n  # Parallelization plan\n  plan(multisession)\n\n  # Define the computation function with checkpointing\n  compute_with_checkpoint &lt;- function(x, checkpoint_dir) {\n    checkpoint_file &lt;- file.path(checkpoint_dir, paste0(\"result_\", x, \".rds\"))\n    if (file.exists(checkpoint_file)) {\n      result &lt;- readRDS(checkpoint_file)\n    } else {\n      Sys.sleep(2)  # Simulate a long computation\n      result &lt;- x^2\n      saveRDS(result, checkpoint_file)\n    }\n    return(result)\n  }\n\n  # Define the pipeline\n  tar_option_set(\n    packages = c(\"future.apply\"),\n    format = \"rds\"\n  )\n\n  list(\n    tar_target(\n      checkpoint_dir,\n      {\n        dir &lt;- \"checkpoints_targets\"\n        dir.create(dir, showWarnings = FALSE)\n        dir\n      },\n      format = \"file\"\n    ),\n    tar_target(\n      data,\n      seq(1, 10),\n      format = \"rds\"\n    ),\n    tar_target(\n      results,\n      future_lapply(data, compute_with_checkpoint, checkpoint_dir = checkpoint_dir),\n      format = \"rds\"\n    ),\n    tar_target(\n      final_save,\n      {\n        saveRDS(results, \"final_results.rds\")\n        results\n      },\n      format = \"rds\"\n    )\n  )\n})\n\n\nlibrary(visNetwork)\nlibrary(targets)\ntar_make()\n\nLoading required package: future\n✔ skipped target checkpoint_dir\n✔ skipped target data\n✔ skipped target results\n✔ skipped target final_save\n✔ skipped pipeline [0.063 seconds]\n\ntar_visnetwork()\n\nLoading required package: future\n\n\n\n\n\n\nThe documentation for targets package can be found here: targets package documentation",
    "crumbs": [
      "R tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R and Posit tutorials",
    "section": "",
    "text": "Preface\nThis Quarto book contains all tutorials created by the Section of Epidemiology. It is deployed from the repository R-tutorials through git backed deployment. You can add more tutorials to this book by cloning this repository and following these steps:\n\n\n\n\n\n\nNote\n\n\n\nThe main branch of the repository has “soft protection” enabled. It is recommended to create a new branch to make any changes and then merging the changes with the main branch after testing to make sure everything renders correctly.\n\n\n\nCreate your tutorial as a QUARTO document in html format.\nAdd your tutorial_name.qmd file to this project folder.\nAdd your tutorial to the _quarto.yml file.\nRun rsconnect::writeManifest() to update the manifest file.\nCommit the changes and push to the repository.\nWait for the deployment to trigger automatically or manually trigger deployment by clicking on the Update Now button under Info on the content page.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis QUARTO book only contains tutorials. There is a lot more documentation (such as Official Posit User Guides, Guides ad How-Tos) in documentation under the posit-team repository.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you create a new session in Workbench, you are creating a RStudio Pro Session. Hence, all the tutorials under the section R/RStudio tutorials will be applicable both on Workbench as well as RStudio on your personal computers (unless stated otherwise in the individual chapters).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "posit infrastructure at VI.html",
    "href": "posit infrastructure at VI.html",
    "title": "1  Posit Infrastructure at VI",
    "section": "",
    "text": "1.1 Posit Infrastructure",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit infrastructure at VI.html#posit-workbench",
    "href": "posit infrastructure at VI.html#posit-workbench",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.2 Posit Workbench",
    "text": "1.2 Posit Workbench\n\nCurrently we have 2 nodes on Workbench.\nA (compute) node is a computer part of a larger set of nodes (a cluster). Besides compute nodes, a cluster comprises one or more login nodes, file server nodes, management nodes, etc. A compute node offers resources such as processors, volatile memory (RAM), permanent disk space (e.g. SSD), accelerators (e.g. GPU) etc.\nTo run a “long” job on Workbench in parallel, you need three things:\n\n\nMultiple cores\nLarge memory (RAM)\nR scripts that have been parallelized.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit infrastructure at VI.html#workbench-infrastructure",
    "href": "posit infrastructure at VI.html#workbench-infrastructure",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.3 Workbench Infrastructure",
    "text": "1.3 Workbench Infrastructure",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit infrastructure at VI.html#background-jobs",
    "href": "posit infrastructure at VI.html#background-jobs",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.4 Background Jobs",
    "text": "1.4 Background Jobs\nNext to your console window you will see background jobs and workbench jobs.\n\n\n\n\n\n\nYou can start a background job by clicking on Start Background Job\n\nThese jobs run in the background of your current session. As soon as you close the session, you abort the job.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit infrastructure at VI.html#workbench-jobs",
    "href": "posit infrastructure at VI.html#workbench-jobs",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.5 Workbench Jobs",
    "text": "1.5 Workbench Jobs\n\n\n\n\n\n\nYou can start a background job by clicking on Start Workbench Job\n\nThese jobs are submitted to the cluster and will not abort when you close the session.\nThe jobs are scheduled through SLURM. When you ask for “large” memory, your job will automatically be submitted on the cn00.posit.vetinst.no node.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit infrastructure at VI.html#workbench-from-terminal",
    "href": "posit infrastructure at VI.html#workbench-from-terminal",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.6 Workbench from Terminal",
    "text": "1.6 Workbench from Terminal\nYou can login to workbench from windows command line (CMD) using the folowing command and then your password\nssh YOUR_VI_NUMBER@workbench.posit.vetinst.no\nSimilarly, you can login directly to the compute directly from CMD too\nssh YOUR_VI_NUMBER@cn00.posit.vetinst.no\nWhen you login, you will end up in your HOME directory. Your home directory and everything within this directory will be the same on both nodes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit infrastructure at VI.html#slurm-jobs-from-terminal",
    "href": "posit infrastructure at VI.html#slurm-jobs-from-terminal",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.7 SLURM jobs from Terminal",
    "text": "1.7 SLURM jobs from Terminal\nYou can schedule SLURM jobs from terminal. Here is a demo SLURM script\n#!/bin/bash  \n#SBATCH --job-name=give_any_jobname \n#SBATCH --output=output_file.out               # Standard output \n#SBATCH --error=error_file.err                 # Standard error \n#SBATCH --ntasks=1                             # Run a single task \n#SBATCH --cpus-per-task=16                     # Number of cores per task (modify based on your need)  \n\n# Execute the R script /home/vetinst.no/viXXXX/rest_of_the_path_of_your_script/script.R\n\n\n\n\n\n\nNote\n\n\n\nThe output_file.out and error_file.err are two text files that will be generated when a SLURM script is run. These files will be generated in the same directory where you have saved your SLURM script file.\n\n\nYou also have to give read, write, and execute permissions to your R script. This can be done by adding\n#!/usr/local/bin/Rscript\nAs the very first line of your R script. Then, in CMD navigate to the location of your R script and type (option 700 means you can do anything with the file or directory and other users have no access to it at all)\nchmod 700 your_script.R\nThen you can run your SLURM script by navigating to the location where you have saved your SLURM script and typing:\nsbatch -w node_you_want_to_run_your_script name_of_slurm_script.slurm\nAs all jobs on workbench now start through SLURM, you will get the output_file.out and error_file.err for every session in the slurm_job_output folder in your home folder.\n\n\n\n\n\n\nImportant\n\n\n\nYou may want to enter another option --mem-per-cpu along with your sbatch command. Here, you provide the amount of RAM you need per CPU to run the task. The default is in MB but you can specify the amount in GB or TB as well (use letters M/G/T for MB, GB and TB respectively).\nFor example the above command can be modified to allocate 100GB to each CPU as follows:\nsbatch -w node_you_want_to_run_your_script --mem-per-cpu 100G name_of_slurm_script.slurm",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "Git backed deployment to posit connect.html",
    "href": "Git backed deployment to posit connect.html",
    "title": "4  Git Backed Deployment to Posit COnnect",
    "section": "",
    "text": "4.1 Overview\nFor public hosted repositories on GitHub it is possible to do a git-backed deployment to Posit Connect\nThe process is described in the official documentation at: Git-Backed Content. Please refer to that link for details.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Git Backed Deployment to Posit COnnect</span>"
    ]
  },
  {
    "objectID": "Git backed deployment to posit connect.html#additional-things-to-consider",
    "href": "Git backed deployment to posit connect.html#additional-things-to-consider",
    "title": "4  Git Backed Deployment to Posit COnnect",
    "section": "4.2 Additional things to consider",
    "text": "4.2 Additional things to consider\nWhen running the R function rsconnect::writeManifest() captures everything that is needed to deploy a given resource. Therefore if the requirements have changed (for example, the code now depends on an additional package), you need to run the function again in order to generate an updated manifest.json file.\nIf you push code changes to GitHub, but not push an update manifest.json file the deployed resource will be the same as before i.e. will reflect what it present in the manifest.json file. Depending on the type of changes made that could cause to crash the resource.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Git Backed Deployment to Posit COnnect</span>"
    ]
  },
  {
    "objectID": "Creating pins on Connect.html",
    "href": "Creating pins on Connect.html",
    "title": "3  Creating Pins on Connect",
    "section": "",
    "text": "3.1 Overview\nPins lets you pin an R or Python object, or a file to a virtual board where you and others access it.\nThe process is described in the official documentation at: Pins. Please refer to that link for details.\nEvery pin lives on a board, so your first step is to create a board object which can be called. This can be done using the function board &lt;- pins::board_connect().\nNote that no arguments are passed to board_connect(). This function is designed to just work for most cases.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Pins on Connect</span>"
    ]
  },
  {
    "objectID": "Creating pins on Connect.html#overview",
    "href": "Creating pins on Connect.html#overview",
    "title": "3  Creating Pins on Connect",
    "section": "",
    "text": "Note\n\n\n\nIf you are using an out-dated version of Rstudio (versions before 2024.04.1) then this function may not work.\nIn that case can specify the auth method to inform how you authenticate to Connect. This can be done by using auth = \"envvar\" as the argument.\nTo do this, set environment variables with usethis::edit_r_profile() to open your .Renviron for editing, and then insert Sys.setenv(CONNECT_API_KEY=\"paste key value\") and Sys.setenv(CONNECT_SERVER=\"https://connect.posit.vetinst.no/\"). Remember to insert a newline character (press ENTER) before saving this file.\nNow you can create the board object using board &lt;- pins::board_connect(auth = \"envvar\")",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Pins on Connect</span>"
    ]
  },
  {
    "objectID": "Creating pins on Connect.html#additional-things-to-consider",
    "href": "Creating pins on Connect.html#additional-things-to-consider",
    "title": "3  Creating Pins on Connect",
    "section": "3.2 Additional things to consider",
    "text": "3.2 Additional things to consider\nRemember, if you’re using git, it’s a good idea to add your .Renviron to your .gitignore to ensure you’re not publishing your API key to your version control system.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Pins on Connect</span>"
    ]
  },
  {
    "objectID": "getting_help_with_R.html",
    "href": "getting_help_with_R.html",
    "title": "6  Getting Help with R",
    "section": "",
    "text": "There are many good resources online that describe a good way to aska question that relates to coding problems. For example, Stack Overflow has a page dedicated to that: How do I ask a good question?. The Rstudio subreddit also has one: How to ask good questions. Additionally, there is the Getting Help section of the R for Data Science book.\nWe encourage you to read those, and the related links on hose pages.\nIn general, a question has to give the people who could answer it enough context so that they can reproduce your eviroment, your code, and your mental model. That would enable them to reproduce the issue as well. Often writing a question like this will help you solve the problem yourself. But in the event that you still need help, consider making a reproducible example.\nA reproducible example (reprex) is a minimal code example that captures the enviroment (i.e. libraries), the data, and the problematic code. Writing this type of a reprex will help you think about the problem more clearly and even may present a solution for you.\nTo share this reprex with other people, you can use the dedicated package that is called reprex and you can install it using install.packages('reprex'). Caling reprex::reprex() in your envrioment then will produce the reproducible example in a sharable format that other people can run in their R console.\nFor example if the code you are running is:\ny &lt;- 1:4\nmean(y)\nThe sharable snippet would be ater running reprex::reprex():\ny &lt;- 1:4\nmean(y)\n#&gt; [1] 2.5\nCreateCreated on 2024-10-07 with reprex v2.1.1\nThis can then be copy/pasted to a console to test it.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Getting Help with R</span>"
    ]
  },
  {
    "objectID": "posit_infrastructure_at_VI.html",
    "href": "posit_infrastructure_at_VI.html",
    "title": "1  Posit Infrastructure at VI",
    "section": "",
    "text": "1.1 Posit Infrastructure",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit_infrastructure_at_VI.html#posit-workbench",
    "href": "posit_infrastructure_at_VI.html#posit-workbench",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.2 Posit Workbench",
    "text": "1.2 Posit Workbench\n\nCurrently we have 2 nodes on Workbench.\nA (compute) node is a computer part of a larger set of nodes (a cluster). Besides compute nodes, a cluster comprises one or more login nodes, file server nodes, management nodes, etc. A compute node offers resources such as processors, volatile memory (RAM), permanent disk space (e.g. SSD), accelerators (e.g. GPU) etc.\nTo run a “long” job on Workbench in parallel, you need three things:\n\n\nMultiple cores\nLarge memory (RAM)\nR scripts that have been parallelized.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit_infrastructure_at_VI.html#workbench-infrastructure",
    "href": "posit_infrastructure_at_VI.html#workbench-infrastructure",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.3 Workbench Infrastructure",
    "text": "1.3 Workbench Infrastructure",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit_infrastructure_at_VI.html#background-jobs",
    "href": "posit_infrastructure_at_VI.html#background-jobs",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.4 Background Jobs",
    "text": "1.4 Background Jobs\nNext to your console window you will see background jobs and workbench jobs.\n\n\n\n\n\n\nYou can start a background job by clicking on Start Background Job\n\nThese jobs run in the background of your current session. As soon as you close the session, you abort the job.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit_infrastructure_at_VI.html#workbench-jobs",
    "href": "posit_infrastructure_at_VI.html#workbench-jobs",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.5 Workbench Jobs",
    "text": "1.5 Workbench Jobs\n\n\n\n\n\n\nYou can start a background job by clicking on Start Workbench Job\n\nThese jobs are submitted to the cluster and will not abort when you close the session.\nThe jobs are scheduled through SLURM. When you ask for “large” memory, your job will automatically be submitted on the cn00.posit.vetinst.no node.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit_infrastructure_at_VI.html#workbench-from-terminal",
    "href": "posit_infrastructure_at_VI.html#workbench-from-terminal",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.6 Workbench from Terminal",
    "text": "1.6 Workbench from Terminal\nYou can login to workbench from windows command line (CMD) using the folowing command and then your password\nssh YOUR_VI_NUMBER@workbench.posit.vetinst.no\nSimilarly, you can login directly to the compute directly from CMD too\nssh YOUR_VI_NUMBER@cn00.posit.vetinst.no\nWhen you login, you will end up in your HOME directory. Your home directory and everything within this directory will be the same on both nodes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "posit_infrastructure_at_VI.html#slurm-jobs-from-terminal",
    "href": "posit_infrastructure_at_VI.html#slurm-jobs-from-terminal",
    "title": "1  Posit Infrastructure at VI",
    "section": "1.7 SLURM jobs from Terminal",
    "text": "1.7 SLURM jobs from Terminal\nYou can schedule SLURM jobs from terminal. Here is a demo SLURM script\n#!/bin/bash  \n#SBATCH --job-name=give_any_jobname \n#SBATCH --output=output_file.out               # Standard output \n#SBATCH --error=error_file.err                 # Standard error \n#SBATCH --ntasks=1                             # Run a single task \n#SBATCH --cpus-per-task=16                     # Number of cores per task (modify based on your need)  \n\n# Execute the R script /home/vetinst.no/viXXXX/rest_of_the_path_of_your_script/script.R\n\n\n\n\n\n\nNote\n\n\n\nThe output_file.out and error_file.err are two text files that will be generated when a SLURM script is run. These files will be generated in the same directory where you have saved your SLURM script file.\n\n\nYou also have to give read, write, and execute permissions to your R script. This can be done by adding\n#!/usr/local/bin/Rscript\nAs the very first line of your R script. Then, in CMD navigate to the location of your R script and type (option 700 means you can do anything with the file or directory and other users have no access to it at all)\nchmod 700 your_script.R\nThen you can run your SLURM script by navigating to the location where you have saved your SLURM script and typing:\nsbatch -w node_you_want_to_run_your_script name_of_slurm_script.slurm\nAs all jobs on workbench now start through SLURM, you will get the output_file.out and error_file.err for every session in the slurm_job_output folder in your home folder.\n\n\n\n\n\n\nImportant\n\n\n\nYou may want to enter another option --mem-per-cpu along with your sbatch command. Here, you provide the amount of RAM you need per CPU to run the task. The default is in MB but you can specify the amount in GB or TB as well (use letters M/G/T for MB, GB and TB respectively).\nFor example the above command can be modified to allocate 100GB to each CPU as follows:\nsbatch -w node_you_want_to_run_your_script --mem-per-cpu 100G name_of_slurm_script.slurm",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Posit Infrastructure at VI</span>"
    ]
  },
  {
    "objectID": "connecting_RStudio_to_GitHub.html",
    "href": "connecting_RStudio_to_GitHub.html",
    "title": "2  Connecting RStudio to GitHub",
    "section": "",
    "text": "2.1 Overview\nIf you are an R user working with some of the code that has been uploaded to the NVI GitHub organization, you will need to set up a way to interact with the service. Below is a recommended way to do so.\nThe assumptions are that you already have installed R and RStudio from the internal NVI software repository. You will additionally need Rtools that you can install on your own. Additionally, this tutorial presumes you already have a GitHub account and you are a member of the NVI organization on GitHub.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Connecting RStudio to GitHub</span>"
    ]
  },
  {
    "objectID": "connecting_RStudio_to_GitHub.html#overview",
    "href": "connecting_RStudio_to_GitHub.html#overview",
    "title": "2  Connecting RStudio to GitHub",
    "section": "",
    "text": "2.1.1 Step 1 - Download and install git\nDownload git from the official website. Install it the usual way. There may be things you would want to configure in the installation, like for example the editor used by git. This is up to your preference.\n\n\n2.1.2 Step 2 - Generate SSH key and add it to GitHub\nRun Git Bash from the Windows start menu. On workbench you can switch to Terminal. Once the terminal opens type ssh-keygen.exe and enter. This will run the program that is used for generating ssh keys. Follow the instructions on the screen and select a good passphrase for your key.\nWhen this is completed, type cat .ssh/id_rsa.pub in the terminal. This is assuming you didn’t give the key a different name during the key generation. This command will print out the public key on the terminal. Copy it and move to the browser.\nOpen https://github.com/settings/keys and then click on New SSH key. Add a title for your key you want to use to recognize the key, and paste the copied public key in the Key field. Click Add SSH key to complete this step.\n\n\n2.1.3 Step 3 - Setup SSH and git in RStudio\nOpen a fresh session of RStudio. See here for some tips on configuring RStudio.\nFor windows machines, in the menu under “Tools” &gt; “Global Options” &gt; “Terminal” set the Shell to Git Bash.\nIn the menu under “Tools” &gt; “Global Options” &gt; “Git/SVN” verify that the path to the private key is showing up.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Connecting RStudio to GitHub</span>"
    ]
  },
  {
    "objectID": "connecting_RStudio_to_GitHub.html#step-4---set-up-github-personal-access-token",
    "href": "connecting_RStudio_to_GitHub.html#step-4---set-up-github-personal-access-token",
    "title": "2  Connecting RStudio to GitHub",
    "section": "2.2 Step 4 - Set up GitHub Personal Access Token",
    "text": "2.2 Step 4 - Set up GitHub Personal Access Token\nA Personal Access Token is not something you necessarily need. It is required for installing private R packages such as the NVIConfig package. You may or may not need this.\nFirst go to https://github.com/settings/tokens and generate a new classic token by clicking on the “Generate new token” dropdown and selecting the classic token. The next page will ask for a lot of options for the token permissions, you will need probably just the repo options on the top of the page, but feel free to select whatever you think is needed for your work. In the Note field add something that will describe the token and click Generate token.\nThe token will be shown in the browser. Keep that open for the time being.\nIn RStudio install devtools and usethis packages: install.packages(“devtools”); usethis: install.packages(“usethis”).\nIn the R terminal in RStudio type and run usethis::edit_r_environ() – this will open a blank .Renviron file (see here if you want to learn more about it).\nIn the empty file write GITHUB_PAT=“” and paste the token from the browser between the quotation marks. Save the .Renviron file. Close RStudio or restart the R session. Close the browser tab that has the token.\nAll done!",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Connecting RStudio to GitHub</span>"
    ]
  },
  {
    "objectID": "anatomy_of_a_R_package.html",
    "href": "anatomy_of_a_R_package.html",
    "title": "3  Anatomy of a R package",
    "section": "",
    "text": "3.1 Overview\nThis tutorial will describe the basics on R packages. How they work, what they are used for, and how to share them.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Anatomy of a R package</span>"
    ]
  },
  {
    "objectID": "anatomy_of_a_R_package.html#tutorial",
    "href": "anatomy_of_a_R_package.html#tutorial",
    "title": "3  Anatomy of a R package",
    "section": "3.2 Tutorial",
    "text": "3.2 Tutorial\n\n3.2.1 What is an R Package?\nAn R package is a convenient way to organize your code, making it easy to share with others. In the R universe, a package is the fundamental unit of shareable code. It bundles together code, data, documentation, and tests. Most packages are available on the Comprehensive R Archive Network (CRAN), and as of date, they host 18308 packages. This is partly why R has become so successful - among these 18000+ packages, chances are someone already solved a problem you are working on, and it is easy to download their package and use it.\nOn your computer, most packages live in the library directory inside the directory where R is installed (typically C:/Program Files/R-&lt;version&gt;). Here, a package is simply a directory that follows a specific structure convention, where functions are stored in the R directory, and their respective help pages are stored in the man or help directory. Other files, such as DESCRIPTION and NAMESPACE are located in the package directory. The DESCRIPTION file lists information about what the package does, who the author is, contact information, versions, license information, package dependencies, and more. The NAMESPACE file makes it possible for packages to communicate to each other. Here, the functions that are defined in the package are exported, which is practice means that they will be available when you load the package. The NAMESPACE file also imports functions from other packages, managing dependencies.\n\n\n\n\n\n\nNote\n\n\n\nPackages are usually only handled in R directly, and not in your own file system! Do not make changes to packages directly in your library!\n\n\nOther files may or may not exist inside the directory of a package. Sometimes a data folder hold some test data, a html directory with a html-file that lists information about the functions inside the package, and others. CRAN packages are usually in a binary format, and the CRAN packages in your default library is uncompressed binary packages. The difference in the directory structure of the package is: - No .R files in the R directory - A Meta directory holding the package metadata (similar to DESCRIPTION above) - Help content appears in help and html - A libs directory that holds the results of compiling the code\nPackage states and structures are complex, and this will not be covered in detail here. You can read more about it here.\n\n\n3.2.2 Where can you find R packages?\nMost packages are available through repositories, where the biggest is CRAN. CRAN is a network of FTP and web servers located around the world, and is coordinated by the R foundation. Packages hosted by CRAN are trustworthy due to extensive documentation criteria and testing to make sure it follows CRAN policies.\nAnother repository for R packages is Bioconductor. This is a topic-specific repository, intended for open source software for bioinformatics. It has its own submission and review process, much like CRAN. The community contributing to Bioconductor is very active hand have several conferences and meetings each year.\nLastly, probably the most popular repository for open source packages is GitHub. This is popular because it makes collaborations easier and it integrates with git. However, no review process is done for packages hosted on GitHub, so use with care!\n\n\n3.2.3 How to install, update, and remove R packages\n\nInstall\n\nCRAN\nPackages hosted by CRAN can be installed by the function install.packages(\"package\"). With default arguments, this will install the package in question at the default library location. To change the library location (if you have a separate library), define the path in the lib argument: install.packages(\"package\", lib = \"path/to/library\"). Specify which mirror you want to use with repo: install.packages(\"package\", repo = \"https://lib.ugent.be/CRAN/\"). You can list available mirrors with getCRANmirrors(). Several packages can be installed with one install.packages call, simply supply a character vector with the package names as the first argument in the function.\n\n\nBioconductor\nPackages hosted by Bioconductor cannot be installed the regular way, unless hosted on both platforms. Bioconductor have their own manager package called BiocManager which you need to install first, as described above (hosted by CRAN). Then, to install specific packages hosted by Bioconductor, use Biocmanager::install(\"package\").\n\n\nGitHub\nPackages hosted on GitHub cannot be installed the regular way, unless they are hosted on CRAN as well (this is the case for several packages). The function devtools::install_github(\"github_user/package_name\") is often used to install packages from GitHub, which originate from the devtools package. The devtools package also has other useful functions, such as install_bioc() and install_cran(), which will install packages from Bioconductor and CRAN, respectively (I am however unsure about the difference between install.packages and Biocmanager::install and these two).\nThe remotes package also has a similar function to install packages from GitHub: remotes::install_github(github_user_name/package_name)\n\n\n\nUpdate and remove\nTo check which packages have updates available, use old.packages(). To update all packages, use update.packages(). If you want to update just one package, simply run the installation function you used to install it again. To remove packages, use remove.packages(\"package\"). This will uninstall the package from the default library, or in the location where R detects the package has been installed.\n\n\n\n3.2.4 How to use packages\n\nGeneral usage\nAfter installing a package, a package can be activated from your library with the library(package) function. This will load the package into memory, making all the functions defined inside it directly available for use. Sometimes, however, you may only need a single function from a package. The function in question may be accessed without loading the package to memory: package_name::function_name(). By using the package::function notation, you avoid potential problems with overlapping function names between packages, and it is easier to identify where the specific function originated.\nOn a side note, the library() function can be used with or without quotation marks, like this: library(\"package\") or library(package). This is, according to Yihui Xie, the original author’s wish to be lazy. The editors of Journal of Statistical Software apparently tries to promote the use of quotations. Or as Yihui put it:\n\n“Apparently, the editors of JSS (Journal of Statistical Software) have been trying to promote the form library(\"foo\") and discourage library(foo), but I do not think it makes much sense now or it will change anything. If it were in the 90’s, I’d wholeheartedly support it. It is simply way too late now. Yes, two extra quotation marks will kill many kittens on this planet.” – Yihui Xie\n\nTo unload a package, the function detach(package) can be used. What is the difference between a library and a package? Think of it this way: A package is a book inside a library. The library function checks a package out of the library, and loads it into memory.\nSome people like to use require() instead of library() to load a package. The difference here is that require() tries to load a package using library(), then returns a logical indicating success or failure, while library() actually load a package. One consequence of using require() is that it does not throw an error if a package you are trying to load is not installed. Therefore, when you try to use a function from the package, it will throw an error, since the package is not installed. In this case, library() would already have thrown an error since the package was not installed. However, sometimes you need to use require() to use a package conditionally, i.e. if the package is not crucial to the script, but optional. If that is the case, the following is useful for detecting that the package is not installed:\nif (require('foo')) {\n    awesome_foo_function() \n} else { \n    warning('You missed an awesome function') \n}\n\nif (!require('foo')) {\n    stop('The package foo was not installed') \n}\nBasically, it sums up to this:\n\n“Try not! Do, or do not. There is no try.” – Jacob Westfall\n\nrequire() breaks one of the fundamental rules of robust software systems: fail early. This is why library() is generally recommended.\n\n\nHelp pages and vignettes\nHelp pages for functions in packages may be accessed like this: ?function_name, ?package::function, ?package, or by the help function: help(function, package = \"package\"). The help pages will describe the general use of a function, give detailed information on what each function argument does, and some simple examples. For a more detailed documentation, many packages have vignettes available. Vignettes are documents that presents package functionality in a more detailed way, often with many examples and plots. The vignettes can be accessed by browseVignettes(package = \"package\") or vignette(package = \"package\").\n\n\n\n3.2.5 How to find the correct R package\nSo how do you find an R package addressing a specific problem, when there are likely over 20.000 packages to choose from? The answer is not simple. Many packages pop up in tutorials that are made for a specific topic, or from talks and workshops around the world. There is also the CRAN Task Views that lists all CRAN packages by topic, making it a little bit easier to sift through. However, by using R and doing tutorials, you will get familiar with many useful packages related to many different problems. Also, asking other R users is one of the quickest ways of finding packages.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Anatomy of a R package</span>"
    ]
  },
  {
    "objectID": "anatomy_of_a_R_package.html#take-home-messages",
    "href": "anatomy_of_a_R_package.html#take-home-messages",
    "title": "3  Anatomy of a R package",
    "section": "3.3 Take-Home messages",
    "text": "3.3 Take-Home messages\n\nAn R package is a convenient way of packing and sharing code\nInstall packages with install.packages(), Biocmanager::install(), or devtools::install_github()\nLoad packages or call functions directly from a package with library() and package::function, avoid using require() unless it is absolutely necessary\nUnload a package with detach()\nAccess help pages with ?package::function or ?function (if package is loaded) or help()\nAsk about packages, or find them in the CRAN Task Views list\nUse tutorials online!",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Anatomy of a R package</span>"
    ]
  },
  {
    "objectID": "anatomy_of_a_R_package.html#resources",
    "href": "anatomy_of_a_R_package.html#resources",
    "title": "3  Anatomy of a R package",
    "section": "3.4 Resources",
    "text": "3.4 Resources\n\nDataCamp - R Packages: A Beginner’s Guide\nWikipedia - R Package\nComprehensive R Archive Network\nList of all R packages hosted by CRAN\nYihui Xie - library() vs. require() in R\nHadley Wickham & Jenny Bryan - R Packages",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Anatomy of a R package</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html",
    "href": "projects_in_RStudio.html",
    "title": "4  Projects in RStudio",
    "section": "",
    "text": "4.1 Agenda",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#agenda",
    "href": "projects_in_RStudio.html#agenda",
    "title": "4  Projects in RStudio",
    "section": "",
    "text": "Go through what an R Project is, and how to start an use one\nWhat is the folder structure?\nHow to handle data inside an R project\nBriefly go through which packages that are recommended and how they are used",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#versions",
    "href": "projects_in_RStudio.html#versions",
    "title": "4  Projects in RStudio",
    "section": "4.2 Versions",
    "text": "4.2 Versions\nThis description of Projects in RStudio is based on the following versions: - R version 4.0.5 - RStudio version 1.4.1717",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#what-is-an-r-project",
    "href": "projects_in_RStudio.html#what-is-an-r-project",
    "title": "4  Projects in RStudio",
    "section": "4.3 What is an R project?",
    "text": "4.3 What is an R project?\nHave you ever wondered where your analysis “lives”, and how to handle overlapping analyses and data? If so, an R project is the thing for you! I want you to consider the following: - When you use R with RStudio, how do you open it? - Do you use only one instance of RStudio for everything? - Do you save your scripts, do they “live” somewhere? - Where do you store your data files and analysis files? - Do you use setwd() when starting RStudio?\nR Projects is a great way to handle the issues that may arise when using a workflow described above. Especially the use of setwd() makes it virtually impossible for anyone else than the original author of the script to make the file paths work. As Jenny Bryan1 put it:\n\nIf the first line of your R script is\nsetwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\")\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥. If the first line of your R script is\nrm(list = ls())\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥.\n\nAn R project is simply a folder on your computer that holds all the files relevant to that particular piece of work. Any scripts run inside an R Project assumes that the working directory is the project folder location, using relative paths. This makes it possible to move the whole project folder anywhere, and it can still be run. Another advantage is that you have full control of package versions and data, assuming you use the relevant packages to handle this. Let’s assume you are analyzing data for a manuscript, where reproducibility is very important. By using an RStudio project, you can gather all necessary data in a data subfolder, and all necessary scripts in a scripts folder, using relative paths. When the paper is published, you can pack the whole project down and share it with anyone who wants to reproduce your analysis! In short, RStudio projects should be reproducible, portable, and self-contained4.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#before-you-start",
    "href": "projects_in_RStudio.html#before-you-start",
    "title": "4  Projects in RStudio",
    "section": "4.4 Before you start",
    "text": "4.4 Before you start\nSome considerations are necessary for optimal and safe use of RStudio projects. In general, you should not be scared of cleaning out the RStudio global environment when working with Projects! I cannot stress this enough. Your analysis should be reproducible to the point where you always start with nothing saved into memory. To be able to force yourself into this way of thinking, I highly recommend to change the following in the Global options in RStudio (“Tools” -&gt; “Global Options”): - Un-tick the “Restore .RData intro workspace at startup” - Set “Save workspace to .RData on exit” to “Never”\nBy doing this, you force yourself to start with a clean slate every time you open the project again. Onward to a more reproducible way of thinking! - Note: This does not affect the saved files and scripts that live inside your project folder, only the data that RStudio has read into memory, i.e. what you see listed under the “Environment” pane in RStudio. - Double-Note: This changes the global setting, which will affect RStudio across all instances on your computer. If you wish to retain this function elsewhere, you may change these settings in “Project Options” instead after creating the project as described below.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#how-to-start-an-rstudio-project",
    "href": "projects_in_RStudio.html#how-to-start-an-rstudio-project",
    "title": "4  Projects in RStudio",
    "section": "4.5 How to start an RStudio Project",
    "text": "4.5 How to start an RStudio Project\nTo start a new RStudio project, open RStudio and click on “File” -&gt; “New Project”. A pop-up box will appear asking you if you want to generate a new directory for the project, if you want to use a pre-existing directory, or to checkout a project from version control history. Generally I would recommend to always start a new project on a clean slate, so pick the “New Directory” option. Next, the menu will ask what kind of project you would like to create. Here, the topmost option “New Project” will suffice for most instances, unless you are creating a package or a Shiny application. Next, you have to give the project a name, and give RStudio the path to where the Project will be stored. You may also choose to tick off “Create git repository” and “Use renv with this project”, but more on that later. - Note: Saving your projects in a dedicated R-Projects folder in a safe location on your computer is recommended! However, some issues may arise if you store it at a location with special symbols or spaces in the path, such as OneDrive - Veterinærinstituttet/R/R_Projects. Try to avoid saving the projects at such a location!\nAfter clicking “Create Project”, the project will be generated and you will be met by an empty project. When a new project is created, the following happens 3:\n\nCreates a project file (with an .Rproj extension) within the project directory. This file contains various project options and can also be used as a shortcut for opening the project directly from the filesystem.\nCreates a hidden directory (named .Rproj.user) where project-specific temporary files (e.g. auto-saved source documents, window-state, etc.) are stored. This directory is also automatically added to .Rbuildignore, .gitignore, etc. if required.\nLoads the project into RStudio and display its name in the Projects toolbar (which is located on the far right side of the main toolbar)\n\nAfter creating a new project, you can create scripts and run analyses!",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#how-to-structure-an-rstudio-project",
    "href": "projects_in_RStudio.html#how-to-structure-an-rstudio-project",
    "title": "4  Projects in RStudio",
    "section": "4.6 How to structure an RStudio project",
    "text": "4.6 How to structure an RStudio project\nHow you structure your project is an important for reproducibility and portability. Below is an example of a structure that is regularly used 5. - Data: This is the directory where the necessary data files are stored. How you structure this directory is up to you. Data is usually .csv, .xlsx, or other formats. - VERY IMPORTANT ABOUT THE DATA DIRECTORY: The data stored here should be regarded as source data, which means that under no circumstance should you be overwriting or editing these files! This is to ensure reproducibility and safety of the project as a whole. - Scripts: This is the directory where the generated scripts are stored, usually with the extension .R or .Rmd. How you structure this is up to you. In most cases I have single scripts that are meant to do one thing (i.e. generate_figures.R,calculate_occurrence.R). Sensible file names are recommended, so that other people may understand what the script does! Also, remember to always use relative paths (i.e. do not reference to paths outside the project folder location), this ensures portability. For more information see the description of the “here” package below. - Functions: This is the directory where self-made functions are stored and called from. This directory may not be necessary for all projects, especially if you only use functions that are defined in packages only. - Output: The output folder for generated data. This is where the results from the scripts is stored, be it figures, tables, or other file types.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#recommended-packages",
    "href": "projects_in_RStudio.html#recommended-packages",
    "title": "4  Projects in RStudio",
    "section": "4.7 Recommended packages",
    "text": "4.7 Recommended packages\nBy far, the most important packages for a reproducible and portable project is renv and here. For detailed information about these packages, click the above links.\n\nrenv The renv package brings dependency management to RStudio projects. This package ensures that all package versions are bundled inside the project itself - making sure that the same versions are used even when updating a package in your main R library. It generates a project library of packages inside the project, and new packages that are installed are only installed there. The workflow with this package is as follows:\n\n\nCall renv::init() to initialize a new project-local environment with a private R library,\nWork in the project as normal, installing and removing new R packages as they are needed in the project,\nCall renv::snapshot() to save the state of the project library to the lockfile (called renv.lock)\nContinue working on your project, installing and updating R packages as needed.\nCall renv::snapshot() again to save the state of your project library if your attempts to update R packages were successful, or call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.\n\nTo install:\ninstall.packages(\"renv\")\n\n# to initialize\nrenv::init()\n\nhere The here package enables easy file referencing in project-oriented workflows. It is a package that contains functions that automatically build the path to the file you want to interact with. Here makes it easy to generate relative paths, which makes the project portable. When you want to read a data file, the following is usually written:\n\nread_delim(\"path/to/datafile.txt\", delim = \"\\t\")\nwith the here package you would write:\nread_delim(here(\"path\",\"to\",\"datafile.txt\"), delim = \"\\t\")\nThe here() function finds your projects files relative to the project’s root location. Also, since there is no need to supply slashes in the path, no issues will arise with the windows standard of backslashes (\\), while R prefers forward slashes (/).\nTo install:\ninstall.packages(\"here\")\n\n# To initialize at currect location\nlibrary(here)",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#packages-to-avoid",
    "href": "projects_in_RStudio.html#packages-to-avoid",
    "title": "4  Projects in RStudio",
    "section": "4.8 Packages to avoid",
    "text": "4.8 Packages to avoid\nSo far, there is only one package that should be avoided when working with Projects in RStudio, the pacman package. Especially, the use of pacman::p_load is to be avoided. This is mainly because the function actually installs with install.packages() and loads the library with library() in one go. If you use it, it will cause issues with the renv management of package versions, because each time you run p_load it will install the newest version of the package in question, breaking the control of package versions.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#version-control",
    "href": "projects_in_RStudio.html#version-control",
    "title": "4  Projects in RStudio",
    "section": "4.9 Version control",
    "text": "4.9 Version control\nVersion control (e.g. git) is a way to ensure that your files are safe, and to track changes throughout the project’s development. RStudio has built-in git functionality, but this may not be optimal for all users. I like to use git outside of RStudio (e.g. git bash), or use the built-in terminal that was implemented in the newer versions of RStudio. However you choose to proceed, using git is good practice, and it makes sure that your files are safe from deletion or accidental changes. More information about git version control and RStudio can be found here. The use of git also allows for upload to f.ex. GitHub, where the code can be published.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#take-home-messages",
    "href": "projects_in_RStudio.html#take-home-messages",
    "title": "4  Projects in RStudio",
    "section": "4.10 Take-Home messages",
    "text": "4.10 Take-Home messages\n\nCreate an RStudio project for each data analysis project\nKeep all data files and scripts used in the analysis inside the project, under version control\nNever overwrite or change the source data files. Save your output in a separate subfolder!\nOnly ever use relative paths, not absolute paths\nUse the here and renv packages to help you out!\nAvoid using pacman",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "projects_in_RStudio.html#resources",
    "href": "projects_in_RStudio.html#resources",
    "title": "4  Projects in RStudio",
    "section": "4.11 Resources",
    "text": "4.11 Resources\n\nJenny Bryan: “Project-oriented workflow”\nR for data science: “Workflow: projects”\nRStudio support: “Using RStudio Projects”\nR-bloggers: “Project-oriented workflow”\nR-bloggers: “RStudio Projects and Working Directories: A Beginner’s Guide”\nJenny Bryan: “Using git with RStudio”\nThe “here” package\nThe “renv” package",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Projects in RStudio</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html",
    "href": "parallelization_on_workbench.html",
    "title": "5  Parallelization on Workbench",
    "section": "",
    "text": "5.1 What is Parallelization?\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can have multiple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here is an example of four quad-core processors for a total of 16 cores in this machine.\nYou can think of this as allowing 16 computations to happen at the same time. Theoretically, your computation would take 1/16 of the time. Historically, R has only utilized one processor, which makes it single-threaded.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html#what-is-parallelization",
    "href": "parallelization_on_workbench.html#what-is-parallelization",
    "title": "5  Parallelization on Workbench",
    "section": "",
    "text": "Important\n\n\n\nThe tutorial shown here is running on the Connect server (and not Workbench) because publication happens on Connect through git backed deployment. Because of this, you will not see the results of parallelization in the outputs of this tutorial as parallelization is not a feature of the connect server. However, if you run the same code on Workbench, you will see the affects of parallelization.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html#the-lapply-function",
    "href": "parallelization_on_workbench.html#the-lapply-function",
    "title": "5  Parallelization on Workbench",
    "section": "5.2 The lapply() function",
    "text": "5.2 The lapply() function\nThe lapply() function has two arguments:\n\nA list, or an object that can be coerced to a list.\nA function to be applied to each element of the list\n\nThe lapply() function works much like a loop. It cycles through each element of the list and applies the supplied function to that element. While lapply() is applying your function to a list element, the other elements of the list are just…sitting around in memory. In the description of lapply(), there’s no mention of the different elements of the list communicating with each other, and the function being applied to a given list element does not need to know about other list elements.\nJust about any operation that is handled by the lapply() function can be parallelized. The idea is that a list object can be split across multiple cores of a processor and then the function can be applied to each subset of the list object on each of the cores. Conceptually, the steps in the parallel procedure are\n\nSplit list X across multiple cores\nCopy the supplied function (and associated environment) to each of the cores\nApply the supplied function to each subset of the list X on each of the cores in parallel\nAssemble the results of all the function evaluations into a single list and return\n\nThe differences between the many packages/functions in R essentially come down to how each of these steps are implemented.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html#the-parallel-package",
    "href": "parallelization_on_workbench.html#the-parallel-package",
    "title": "5  Parallelization on Workbench",
    "section": "5.3 The Parallel Package",
    "text": "5.3 The Parallel Package\nThe parallel package which comes with your R installation.\nThe mclapply() function essentially parallelizes calls to lapply(). The first two arguments to mclapply() are exactly the same as they are for lapply(). However, mclapply() has further arguments (that must be named), the most important of which is the mc.cores argument which you can use to specify the number of processors/cores you want to split the computation across. For example, if your machine has 4 cores on it, you might specify mc.cores = 4 to break your parallelize your operation across 4 cores (although this may not be the best idea if you are running other operations in the background besides R).\nBriefly, your R session is the main process and when you call a function like mclapply(), you fork a series of sub-processes that operate independently from the main process (although they share a few low-level features). These sub-processes then execute your function on their subsets of the data, presumably on separate cores of your CPU. Once the computation is complete, each sub-process returns its results and then the sub-process is killed. The parallel package manages the logistics of forking the sub-processes and handling them once they’ve finished.\n\n\n\n\n\n\nCaution\n\n\n\nBecause of the use of the fork mechanism, the mc* functions are generally not available to users of the Windows operating system.\n\n\nThe first thing you might want to check with the parallel package is if your computer in fact has multiple cores that you can take advantage of.\n\nlibrary(parallel)\nparallel::detectCores()\n\n[1] 64\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn general, the information from detectCores() should be used cautiously as obtaining this kind of information from Unix-like operating systems is not always reliable. If you are going down this road, it’s best if you get to know your hardware better in order to have an understanding of how many CPUs/cores are available to you.\n\n\n\n5.3.1 mclapply()\n\nset.seed(1)\n# Create a dataframe\ndf &lt;- data.frame(replicate(1000, rnorm(10000)))\n\n# Using lapply() to find mean of each row\ns &lt;- system.time({\n  list_means_1 &lt;- lapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])))\n})\nprint(s)\n\n   user  system elapsed \n 45.157   0.006  45.201 \n\n\nNote that in the system.time() output in first case, the user time and the elapsed time are roughly the same, which is what we would expect because there was no parallelization.\n\nlibrary(parallel)\n\n# Using mclapply() to find mean of each row\nnumberOfCores &lt;- 4\ns &lt;- system.time({\n  list_means_2 &lt;- parallel::mclapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])), mc.cores = numberOfCores)\n})\nprint(s)\n\n   user  system elapsed \n 34.376   0.259  11.865 \n\n\nYou’ll notice that the the elapsed time is now less than the user time. However, in general, the elapsed time will not be 1/4th of the user time, which is what we might expect with 4 cores if there were a perfect performance gain from parallelization.\nR keeps track of how much time is spent in the main process and how much is spent in any child processes.\n\ns[\"user.self\"]  # Main process\n\nuser.self \n    0.004 \n\ns[\"user.child\"] # Child processes\n\nuser.child \n    34.372 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nOne advantage of serial computations is that it allows you to better keep a handle on how much memory your R job is using. When executing parallel jobs via mclapply() it’s important to pre-calculate how much memory all of the processes will require and make sure this is less than the total amount of memory on your computer.\n\n\nThe mclapply() function is useful for iterating over a single list or list-like object. If you have to iterate over multiple objects together, you can use mcmapply(), which is the the multi-core equivalent of the mapply() function.\n\n\n5.3.2 Error Handling\nThis error handling behavior is a significant difference from the usual call to lapply(). With lapply(), if the supplied function fails on one component of the list, the entire function call to lapply() fails and you only get an error as a result.\nWith mclapply(), when a sub-process fails, the return value for that sub-process will be an R object that inherits from the class \"try-error\", which is something you can test with the inherits() function. Conceptually, each child process is executed with the try() function wrapped around it. The code below deliberately causes an error in the 3 element of the list.\n\nr &lt;- parallel::mclapply(1:5, function(i) {\n        if(i == 3L)\n                stop(\"error in this process!\")\n        else\n                return(\"success!\")\n}, mc.cores = 5)\n\nWarning in parallel::mclapply(1:5, function(i) {: scheduled core 3 encountered\nerror in user code, all values of the job will be affected\n\n\nHere we see there was a warning but no error in the running of the above code. We can check the return value.\n\nstr(r)\n\nList of 5\n $ : chr \"success!\"\n $ : chr \"success!\"\n $ : 'try-error' chr \"Error in FUN(X[[i]], ...) : error in this process!\\n\"\n  ..- attr(*, \"condition\")=List of 2\n  .. ..$ message: chr \"error in this process!\"\n  .. ..$ call   : language FUN(X[[i]], ...)\n  .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n $ : chr \"success!\"\n $ : chr \"success!\"\n\n\nNote that the 3rd list element in r is different.\n\nclass(r[[3]])\n\n[1] \"try-error\"\n\ninherits(r[[3]], \"try-error\")\n\n[1] TRUE\n\n\nWhen running code where there may be errors in some of the sub-processes, it’s useful to check afterwards to see if there are any errors in the output received.\n\nbad &lt;- sapply(r, inherits, what = \"try-error\")\nbad\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n\n5.3.3 Generating Random Numbers\n\nset.seed(1)\nr &lt;- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n\nList of 5\n $ : num [1:3] -0.943 0.831 1.082\n $ : num [1:3] -0.5895 0.0173 -0.5033\n $ : num [1:3] -0.511 0.203 0.187\n $ : num [1:3] 0.106 -1.4404 0.0288\n $ : num [1:3] -0.31 0.52 -0.446\n\n\nHowever, the above expression is not reproducible because the next time you run it, you will get a different set of random numbers. You cannot simply call set.seed() before running the expression as you might in a non-parallel version of the code.\nThe parallel package provides a way to reproducibly generate random numbers in a parallel environment via the “L’Ecuyer-CMRG” random number generator. Note that this is not the default random number generator so you will have to set it explicitly.\n\nRNGkind(\"L'Ecuyer-CMRG\")\nset.seed(1)\nr &lt;- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n\nList of 5\n $ : num [1:3] -0.485 -0.626 -0.873\n $ : num [1:3] -1.86 -1.825 -0.995\n $ : num [1:3] 1.177 1.472 -0.988\n $ : num [1:3] 0.984 1.291 0.459\n $ : num [1:3] 1.43 -1.137 0.844\n\n\nmclapply() documentation can be found here: mcapply()\n\n\n5.3.4 The ParLapply() function\nUsing the forking mechanism on your computer is one way to execute parallel computation but it’s not the only way that the parallel package offers. Another way to build a “cluster” using the multiple cores on your computer is via sockets. A is simply a mechanism with which multiple processes or applications running on your computer (or different computers, for that matter) can communicate with each other. With parallel computation, data and results need to be passed back and forth between the parent and child processes and sockets can be used for that purpose.\n\nclu &lt;- parallel::makeCluster(4)\n\nThe clu object is an abstraction of the entire cluster and is what we’ll use to indicate to the various cluster functions that we want to do parallel computation.\nTo do an lapply() operation over a socket cluster we can use the parLapply() function.\n\nlist_means_3 &lt;- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n\nError in checkForRemoteErrors(val): 4 nodes produced errors; first error: object of type 'closure' is not subsettable\n\n\nUnfortunately, that there’s an error in running this code. The reason is that while we have loaded the df data into our R session, the data is not available to the independent child processes that have been spawned by the makeCluster() function. The socket approach launches a new version of R on each core whereas the forking approach copies the entire current version of R and moves it to a new core.\nThe data, and any other information that the child process will need to execute your code, needs to be exported to the child process from the parent process via the clusterExport() function. The need to export data is a key difference in behavior between the “multicore” approach and the “socket” approach.\n\nparallel::clusterExport(clu, \"df\")\n\nThe second argument to clusterExport() is a character vector, and so you can export an arbitrary number of R objects to the child processes. You should be judicious in choosing what you export simply because each R object will be replicated in each of the child processes, and hence take up memory on your computer.\n\nlist_means_3 &lt;- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n\nOnce you’ve finished working with your cluster, it’s good to clean up and stop the cluster child processes (quitting R will also stop all of the child processes).\n\nparallel::stopCluster(clu)\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes we will also need to load the packages in individual child processes. This can be done by using clusterEvalQ . For example:\nparallel::clusterEvalQ(clu, {\n  library(ggplot2)\n  library(stringr)\n})\n\n\nParLapply() is a part of clusterApply() family of functions. The documentation can be found here: clusterApply()",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html#foreach-and-doparallel-package",
    "href": "parallelization_on_workbench.html#foreach-and-doparallel-package",
    "title": "5  Parallelization on Workbench",
    "section": "5.4 foreach and doParallel Package",
    "text": "5.4 foreach and doParallel Package\nThe normal for loop in R looks like:\n\nfor (i in 1:3) {\n  print(sqrt(i))\n}\n\n[1] 1\n[1] 1.414214\n[1] 1.732051\n\n\nThe foreach method is similar, but uses the sequential %do% operator to indicate an expression to run. Note the difference in the returned data structure.\n\nlibrary(foreach)\nforeach (i=1:3) %do% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n\n\n5.4.1 %dopar% operator\nIn addition, foreach supports a parallelizable operator %dopar% from the doParallel package. This allows each iteration through the loop to use different cores or different machines in a cluster.\n\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\ndoParallel::registerDoParallel(4) \nforeach (i=1:5) %dopar% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 2.236068\n\n\nTo simplify output, foreach has the .combine parameter that can simplify return values\n\nforeach (i=1:3, .combine=c) %dopar% {\n  sqrt(i)\n}\n\n[1] 1.000000 1.414214 1.732051\n\n\nforeach also has the .rbind parameter that can return a dataframe\n\nforeach (i=1:3, .combine=rbind) %dopar% {\n  sqrt(i)\n}\n\n             [,1]\nresult.1 1.000000\nresult.2 1.414214\nresult.3 1.732051\n\n\nThe doParallel vignette on CRAN shows a much more realistic example, where one can use %dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined. Here use the iris data set to do a parallel bootstrap:\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %dopar% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 11.478   0.302   3.508 \n\n\nAnd compare that to what it takes to do the same analysis in serial:\n\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %do% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 12.193   0.001  12.203 \n\n\nWhen we’re done, we will clean up the cluster:\n\ndoParallel::stopImplicitCluster()\n\n\n\n5.4.2 %dorng% operator\nstandard %dopar% loops are not reproducible:\nFirst, let’s set the RNGkind back to default\n\nRNGkind(\"default\")\n\nNow register a new cluster\n\ndoParallel::registerDoParallel(4)\n\n\nset.seed(123)\nres &lt;- foreach(i=1:5) %dopar% { runif(3) }\nset.seed(123)\nres2 &lt;- foreach(i=1:5) %dopar% { runif(3) }\nidentical(res, res2)\n\n[1] FALSE\n\n\nThe doRNG package provides convenient ways to implement reproducible parallel foreach loops, independently of the parallel backend used to perform the computation.\n\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nset.seed(123)\nres &lt;- foreach(i=1:5) %dorng% { runif(3) }\nset.seed(123)\nres2 &lt;- foreach(i=1:5) %dorng% { runif(3) }\nidentical(res, res2)\n\n[1] TRUE\n\n\nWhen we’re done, we will clean up the cluster:\n\ndoParallel::stopImplicitCluster()\n\nThe doParallel documentation can be found here: doParallel",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html#the-future-package",
    "href": "parallelization_on_workbench.html#the-future-package",
    "title": "5  Parallelization on Workbench",
    "section": "5.5 The future Package",
    "text": "5.5 The future Package\nThe future package defines plans to specify how computations are executed. A plan can use multiple cores, separate R sessions, or even remote systems.\n\nlibrary(future)\nplan(multisession, workers = 4)\n\n# Define a future\nf &lt;- future::future({ sum(1:1e6) })\n# Retrieve the result\nresult &lt;- future::value(f)\nprint(result)\n\n[1] 500000500000\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe could have used plan(multicore) instead of plan(multisession) if we were working directly in R and not RStudio. However, plan(multicore) will only work in a Linux/macOS environment\n\n\nThe future package is a lot more comprehensive but beyond the scope of discussion for this basic tutorial. If you are interested, the documentation can be found here: future package\nThis is how we can use it instead of lapply\n\nlibrary(future.apply)\nplan(multisession, workers = 4)\nresult &lt;- future.apply::future_lapply(1:5, function(x) x^2)\nresult\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nMultisession runs background R sessions on the current machine. For large parallelizations, we should run future_lapply by defining a cluster manually. That way we can run it on external R sessions on current, local, and/or remote machines.\n\nlibrary(parallel)\nclu &lt;- parallel::makeCluster(4)\nplan(cluster, workers = clu)\nresult &lt;- future_lapply(1:10, function(x) x^2)\nstopCluster(clu)\n\nDocumentation for future.apply family of functions can be found here: future.apply documentation\n\n5.5.1 Integration with SLURM\nFuture works very well with SLURM integration. The future.batchtools package extends the future ecosystem for SLURM. Here is an example code:\nlibrary(future.batchtools)\nplan(batchtools_slurm, template = \"sumbit_job.slurm\")\n\nf &lt;- future::future({ Sys.sleep(10); sum(1:1e6) })\nresult &lt;- future::value(f)\nYou need a SLURM template file (submit_job.slurm) that specifies job parameters (e.g., cores, memory).",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html#other-ways-to-parallelize-r-code",
    "href": "parallelization_on_workbench.html#other-ways-to-parallelize-r-code",
    "title": "5  Parallelization on Workbench",
    "section": "5.6 Other ways to parallelize R code",
    "text": "5.6 Other ways to parallelize R code\nThere are several other ways to parallelize your code. If you are looking for more packages, some of those are mentioned here along with their documentation. Some of these are extensions of packages already mentioned while others introduce different ways to parallelize.\n\nfurrr package\ndoFuture package\nRcppParallel package\nparallelMap package\nbatchtools package",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "parallelization_on_workbench.html#saving-results-while-parallelizing",
    "href": "parallelization_on_workbench.html#saving-results-while-parallelizing",
    "title": "5  Parallelization on Workbench",
    "section": "5.7 Saving results while parallelizing",
    "text": "5.7 Saving results while parallelizing\nWhen running large computations, it may be helpful to save results iteratively or as checkpoints to avoid data loss in case of interruptions. Here is an example of saving results iteratively using the foreach and doParallel libraries\n\nlibrary(foreach)\nlibrary(doParallel)\n\n# Register parallel backend\ndoParallel::registerDoParallel(4)\n\n# Parallel computation and saving results\nresults &lt;- foreach(i = 1:100, .combine = c) %dopar% {\n  # Your computation\n  Sys.sleep(0.1) # Simulates time-consuming computation\n  result &lt;- i^2\n  \n  # Save intermediate results\n  saveRDS(result, file = paste0(\"for_each/result_\", i, \".rds\"))\n  result\n}\n\ndoParallel::stopImplicitCluster()\n\nHere is another example using future.apply library\n\nlibrary(future.apply)\n\nplan(multisession, workers = 4)\n\n# Function with intermediate saving\nsafe_compute &lt;- function(i) {\n  result &lt;- i^2\n  saveRDS(result, file = paste0(\"future_apply/partial_result_\", i, \".rds\"))\n  return(result)\n}\n\n# Run computation\nresults &lt;- future.apply::future_lapply(1:100, safe_compute)\n\n# Aggregate saved results\nfinal_results &lt;- unlist(results)\nsaveRDS(final_results, file = \"future_apply/final_results.rds\")\n\n\n5.7.1 Designing a function to restore progress:\nHere is the “structure” of a function that can be used to restore your progress.\n\nlibrary(future.apply)\n\n# Define checkpoint directory\ncheckpoint_dir &lt;- \"checkpoints_parallel\"\ndir.create(checkpoint_dir, showWarnings = FALSE)\n\n# Set up parallel plan\nplan(multisession, workers = 4)\n\n# Function to perform computations with checkpoints\ncompute_task &lt;- function(i) {\n  checkpoint_file &lt;- file.path(checkpoint_dir, paste0(\"result_\", i, \".rds\"))\n  \n  if (file.exists(checkpoint_file)) {\n    # Restore from checkpoint\n    result &lt;- readRDS(checkpoint_file)\n  } else {\n    # Perform the computation\n    Sys.sleep(1)  # Simulates a time-consuming task\n    result &lt;- i*2\n    \n    # Save checkpoint\n    saveRDS(result, checkpoint_file)\n  }\n  return(result)\n}\n\n# Parallel computation with checkpoints\nresults &lt;- future.apply::future_lapply(1:10, compute_task)\n\n# Aggregate results\nfinal_results &lt;- unlist(results)\nprint(final_results)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\n\n5.7.2 Targets package for efficient checkpoint management\nThis package is a pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.\n\nlibrary(targets)\ntar_script({\n  library(future.apply)\n\n  # Parallelization plan\n  plan(multisession)\n\n  # Define the computation function with checkpointing\n  compute_with_checkpoint &lt;- function(x, checkpoint_dir) {\n    checkpoint_file &lt;- file.path(checkpoint_dir, paste0(\"result_\", x, \".rds\"))\n    if (file.exists(checkpoint_file)) {\n      result &lt;- readRDS(checkpoint_file)\n    } else {\n      Sys.sleep(2)  # Simulate a long computation\n      result &lt;- x^2\n      saveRDS(result, checkpoint_file)\n    }\n    return(result)\n  }\n\n  # Define the pipeline\n  tar_option_set(\n    packages = c(\"future.apply\"),\n    format = \"rds\"\n  )\n\n  list(\n    tar_target(\n      checkpoint_dir,\n      {\n        dir &lt;- \"checkpoints_targets\"\n        dir.create(dir, showWarnings = FALSE)\n        dir\n      },\n      format = \"file\"\n    ),\n    tar_target(\n      data,\n      seq(1, 10),\n      format = \"rds\"\n    ),\n    tar_target(\n      results,\n      future_lapply(data, compute_with_checkpoint, checkpoint_dir = checkpoint_dir),\n      format = \"rds\"\n    ),\n    tar_target(\n      final_save,\n      {\n        saveRDS(results, \"final_results.rds\")\n        results\n      },\n      format = \"rds\"\n    )\n  )\n})\n\n\nlibrary(visNetwork)\nlibrary(targets)\ntar_make()\n\nLoading required package: future\n✔ skipped target checkpoint_dir\n✔ skipped target data\n✔ skipped target results\n✔ skipped target final_save\n✔ skipped pipeline [0.063 seconds]\n\ntar_visnetwork()\n\nLoading required package: future\n\n\n\n\n\n\nThe documentation for targets package can be found here: targets package documentation",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parallelization on Workbench</span>"
    ]
  },
  {
    "objectID": "RStudio_tips_and_tricks.html",
    "href": "RStudio_tips_and_tricks.html",
    "title": "7  RStudio Tips and Tricks",
    "section": "",
    "text": "7.1 Overview\nThis is a guide on how to do different things in RStudio to make your work easier and simpler. Some of it is good practices, some of it is just my preferences, but it may be useful for other people too.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>RStudio Tips and Tricks</span>"
    ]
  },
  {
    "objectID": "RStudio_tips_and_tricks.html#overview",
    "href": "RStudio_tips_and_tricks.html#overview",
    "title": "7  RStudio Tips and Tricks",
    "section": "",
    "text": "7.1.1 RStudio settings\nUnder the menu “Tools” &gt; “Global Options” &gt; “General” &gt; “Basic”:\nThe R session section should be set to restore only opened source documents. This is because sometimes you can be drafting a code and forget to save it. It may be useful, or it may be not useful. But it is a good way to see what was going on.\nThe Workspace section, set it up to not restore or save .Rdata. This is because it is good to have an empty environment when you start RStudio.\nUnder the “Tools menu” &gt; “Global Options” &gt; “Code” &gt; “Editing”:\nCheck to use the native pipe operator |&gt;. This is because most of the time you don’t need the magrittr pipe %&gt;%, and/or maybe you don’t want to load a package just to have the pipe available. There is some difference how the two pipes behave, just to be aware of it:\n❯ iris %&gt;% head()\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n❯ iris %&gt;% head\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n❯ iris |&gt; head()\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n❯ iris |&gt; head\nError in head : \n  The pipe operator requires a function call as RHS (&lt;input&gt;:1:9)\nThe shortcut to type the pipe is always useful regardless of which pipe you are using: Ctrl+Shift+M.\nUnder the menu “Tools” &gt; “Global Options” &gt; “Code” &gt; “Display”:\nCheck Highlight R function calls. Great to be able to see which part of your code is actually a function name.\nCheck Use rainbow parentheses. Great to be abel to see the matching parentheses especially in long chunks of code and in Shiny.\nCheck Show margin and set at 80 characters. This will show a vertical line in the code to help you keep readability in mind when writing long lines of code or comments.\n\n\n7.1.2 RStudio Workspace Panes\nThe default settings for the Workspace Panes in Rstudio has the code editor on the left side.\nA good improvement on that is to have the editor on the right side of the workspace and the console/terminal on the left. The bottom pane on the right side should keep just the history tab, whereas all the other tabs go in the bottom left pane. That way, you can expand the code editing pane to the whole right side of the screen without sacrificing too much functionality. You don’t need to check the code history tab too much when working, if at all.\nTo set this up go to “Tools” &gt; “Pane Layout”. It should look like this:\n\n\n\nPane Layout\n\n\nThis is also focus your vision to the center of the screen as most typing in the code editing panel will happen in the centre of the screen within the 80 character margin.",
    "crumbs": [
      "R/RStudio tutorials",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>RStudio Tips and Tricks</span>"
    ]
  },
  {
    "objectID": "Creating_pins_on_Connect.html",
    "href": "Creating_pins_on_Connect.html",
    "title": "8  Creating Pins on Connect",
    "section": "",
    "text": "8.1 Overview\nPins lets you pin an R or Python object, or a file to a virtual board where you and others access it.\nThe process is described in the official documentation at: Pins. Please refer to that link for details.\nEvery pin lives on a board, so your first step is to create a board object which can be called. This can be done using the function board &lt;- pins::board_connect().\nNote that no arguments are passed to board_connect(). This function is designed to just work for most cases.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Creating Pins on Connect</span>"
    ]
  },
  {
    "objectID": "Creating_pins_on_Connect.html#overview",
    "href": "Creating_pins_on_Connect.html#overview",
    "title": "8  Creating Pins on Connect",
    "section": "",
    "text": "Note\n\n\n\nIf you are using an out-dated version of Rstudio (versions before 2024.04.1) then this function may not work.\nIn that case can specify the auth method to inform how you authenticate to Connect. This can be done by using auth = \"envvar\" as the argument.\nTo do this, set environment variables with usethis::edit_r_profile() to open your .Renviron for editing, and then insert Sys.setenv(CONNECT_API_KEY=\"paste key value\") and Sys.setenv(CONNECT_SERVER=\"https://connect.posit.vetinst.no/\"). Remember to insert a newline character (press ENTER) before saving this file.\nNow you can create the board object using board &lt;- pins::board_connect(auth = \"envvar\")",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Creating Pins on Connect</span>"
    ]
  },
  {
    "objectID": "Creating_pins_on_Connect.html#additional-things-to-consider",
    "href": "Creating_pins_on_Connect.html#additional-things-to-consider",
    "title": "8  Creating Pins on Connect",
    "section": "8.2 Additional things to consider",
    "text": "8.2 Additional things to consider\nRemember, if you’re using git, it’s a good idea to add your .Renviron to your .gitignore to ensure you’re not publishing your API key to your version control system.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Creating Pins on Connect</span>"
    ]
  },
  {
    "objectID": "Git_backed_deployment_to_posit_connect.html",
    "href": "Git_backed_deployment_to_posit_connect.html",
    "title": "9  Git Backed Deployment to Posit Connect",
    "section": "",
    "text": "9.1 Overview\nFor public hosted repositories on GitHub it is possible to do a git-backed deployment to Posit Connect\nThe process is described in the official documentation at: Git-Backed Content. Please refer to that link for details.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Git Backed Deployment to Posit Connect</span>"
    ]
  },
  {
    "objectID": "Git_backed_deployment_to_posit_connect.html#additional-things-to-consider",
    "href": "Git_backed_deployment_to_posit_connect.html#additional-things-to-consider",
    "title": "9  Git Backed Deployment to Posit Connect",
    "section": "9.2 Additional things to consider",
    "text": "9.2 Additional things to consider\nWhen running the R function rsconnect::writeManifest() captures everything that is needed to deploy a given resource. Therefore if the requirements have changed (for example, the code now depends on an additional package), you need to run the function again in order to generate an updated manifest.json file.\nIf you push code changes to GitHub, but not push an update manifest.json file the deployed resource will be the same as before i.e. will reflect what it present in the manifest.json file. Depending on the type of changes made that could cause to crash the resource.",
    "crumbs": [
      "Posit Connect",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Git Backed Deployment to Posit Connect</span>"
    ]
  }
]